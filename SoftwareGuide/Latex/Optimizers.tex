
\index{itk::Optimizer}
\index{itk::Single\-Valued\-NonLinear\-Optimizer}


\begin{figure}
\center
\includegraphics[width=\textheight,angle=90]{OptimizersHierarchy.eps}
\itkcaption[Class diagram of the Optimizer hierarchy]{Class diagram of the
optimizers hierarchy.}
\label{fig:OptimizersHierarchy}
\end{figure}

Optimization algorithms are encapsulated as \doxygen{Optimizer} objects
within ITK. Optimizers are generic and can be used for applications other than
registration.  Within the registration framework, subclasses of
\doxygen{SingleValuedNonLinearOptimizer} are used to optimize the metric
criterion with respect to the transform parameters.

\index{itk::Optimizer!SetInitialPosition()}
\index{itk::Optimizer!StartOptimization()}
\index{itk::Optimizer!GetCurrentPosition()}

The basic input to an optimizer is a cost function object. In the context
of registration, \doxygen{ImageToImageMetric} provides this functionality.
The initial parameters are set using \code{SetInitialPosition()} and
the optimization algorithm is invoked by \code{StartOptimization()}.
Once the optimization has finished, the final parameters can be obtained
using \code{GetCurrentPosition()}.

\index{itk::Optimizer!SetScales()}
Some optimizers also allow rescaling of their individual parameters. This
is convenient for normalizing parameters spaces where some parameters
have different dynamic ranges. For example, the first parameter of
\doxygen{Euler2DTransform} represents an angle while the last two parameters
the translation. A unit change in angle has a much greater impact on
an image than a unit change in translation. This difference in scale appears
as long narrow valleys in the search space making the optimization problem
diffcult. Rescaling the translation parameters can help to fix this problem.
Scales are represented as an \doxygen{Array} of doubles and set defined using
\code{SetScales()}.

The types of SingleValuedNonLinearOptimizers currently available
in ITK are:

\index{itk::Amoeba\-Optimizer}
\index{itk::Conjugate\-Gradient\-Optimizer}
\index{itk::Gradient\-Descent\-Optimizer}
\index{itk::Quaternion\-Rigid\-Transform\-Gradient\-Descent\-Optimizer}
\index{itk::LBFGS\-Optimizer}
\index{itk::One\-Plus\-One\-Evolutionary\-Optimizer}
\index{itk::Regular\-Step\-Gradient\-Descent\-Optimizer}
\index{itk::Versor\-Transform\-Optimizer}
\index{itk::Levenberg\-Marquardt\-Optimizer}

\begin{itemize}

\item \textbf{Amoeba}: Nelder-Meade downhill simplex.  This optimizer is
actually implemented in the \code{vxl/vnl} numerics toolkit.  The ITK class
\doxygen{AmoebaOptimizer} is merely an adaptor class.

\item \textbf{Conjugate Gradient}: Fletcher-Reeves form 
of the conjugate gradient with or without preconditioning
(\doxygen{ConjugateGradientOptimizer}). It is also an adaptor to an optimizer in
\code{vnl}.

\item \textbf{Gradient Descent}: Advances parameters in the direction of the
gradient where the step size is governed by a learning rate (\doxygen{GradientDescentOptimizer}). 

\item \textbf{Quaternion Rigid Transform Gradient Descent}: 
A specialized version of GradientDescentOptimizer for
QuaternionRigidTransform parameters, where the parameters representing
the quaternion are normalized to a magnitude of one at each iteration to
represent a pure rotation (\doxygen{QuaternionRigidTransformGradientDescent}).

\item \textbf{LBFGS}: Limited memory Broyden, Fletcher, Goldfarb
and Shannon minmization. It is an adaptor to an optimizer in \code{vnl}
(\doxygen{LBFGSOptimizer}).

\item \textbf{One Plus One Evolutionary}: Strategy that simulates the
biological evolution of a set of samples in the search space. This optimizer
is mainly used in the process of bias correction of MRI images
(\doxygen{OnePlusOneEvolutionaryOptimizer.}).

\item \textbf{Regular Step Gradient Descent}: Advances parameters in the
direction of the gradient where a bipartition scheme is used to compute
the step size (\doxygen{RegularStepGradientDescentOptimizer}). 

\item \textbf{Versor Transform Optimizer}: A specialized version of the 
RegularStepGradientDescentOptimizer for VersorTransform
parameters, where the current rotation is composed with the gradient rotation
to produce the new rotation vector. It follows the definition of versor
gradients defined by Hamilton~\cite{Hamilton1866}
(\doxygen{VersorTransformOptimizer}).

\end{itemize}

A parallel hierarchy exists for optimizing multiple-valued cost functions. The
base optimizer in this branch of the hierarchy is the
\doxygen{MultipleValuedNonLinearOptimizer} whose only current derived class
is:

\begin{itemize}

\item \textbf{Levenberg Marquardt}: Non-linear least squares minimization.
Adapted to an optimizer in \code{vnl} (\doxygen{LevenbergMarquardtOptimizer}).

\end{itemize}


Figure \ref{fig:OptimizersHierarchy} illustrates the full class hierarchy of
optimizers in ITK. Optimizers in the lower right corner are adaptor classes
to optimizers existing in the \code{vxl/vnl} numerics toolkit. The optimizers
interact with the \doxygen{CostFunction} class. In the registration framework
this cost function is reimplemented in the form of ImageToImageMetric.





